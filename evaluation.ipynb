{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'yolov10FX') # if you're working with yolov8 models, please change the path to 'yolov8FX'\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ultralytics import YOLOv10\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "import ijson\n",
    "from utils.monitor_construction import features_clustering_by_k_start, monitor_construction_from_features\n",
    "from utils.evaluation import get_distance_dataset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fpr(conf, label):\n",
    "    ood_indicator = (label != -1).astype(int)\n",
    "    fpr_list, tpr_list, _ = metrics.roc_curve(ood_indicator, conf)\n",
    "    return fpr_list[np.argmax(tpr_list >= 0.95)]\n",
    "    \n",
    "def load_json_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def filter_and_save_json(json_path, threshold=0.25):\n",
    "    data = load_json_data(json_path)\n",
    "    \n",
    "    filtered_data = [entry for entry in data if entry['score'] >= threshold]\n",
    "    \n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(filtered_data, f)\n",
    "\n",
    "def json2npy(model_type, id, dataset_name, threshold=0.25):\n",
    "    base_path = os.path.join(\"feats\", f\"{model_type}_{id}\", dataset_name)\n",
    "    logits_file = os.path.join(base_path, f\"logits_{threshold}.npy\")\n",
    "    labels_file = os.path.join(base_path, f\"labels_{threshold}.npy\")\n",
    "\n",
    "    if os.path.isfile(logits_file) and os.path.isfile(labels_file):\n",
    "        return np.load(logits_file), np.load(labels_file)\n",
    "\n",
    "    json_file = os.path.join(base_path, \"predictions.json\")\n",
    "    \n",
    "    logits, labels = [], []\n",
    "    with open(json_file, 'r') as f:\n",
    "        for obj in ijson.items(f, 'item'):\n",
    "            logits.append(obj[\"logits\"])\n",
    "            labels.append(obj[\"category_id\"])\n",
    "\n",
    "    logits = np.array(logits, dtype=np.float32)\n",
    "    labels = np.array(labels, dtype=np.int32)\n",
    "\n",
    "    np.save(logits_file, logits)\n",
    "    np.save(labels_file, labels)\n",
    "    return logits, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def inference_dataset(dataset_name, model_name):\n",
    "    model = YOLOv10(f'models/{model_name}/best.pt')\n",
    "    model_type, id = model_name.split('_')\n",
    "    split = \"train\" if dataset_name == f\"{id}-train\" else \"val\"\n",
    "    data_path = f'datasets/{id}/dataset.yaml' if dataset_name.startswith(id) else f'datasets/{dataset_name}/dataset.yaml'\n",
    "    OUTPUT_PATH = f'feats/{model_name}'\n",
    "    output_folder = os.path.join(OUTPUT_PATH, dataset_name)\n",
    "    if os.path.exists(output_folder):\n",
    "        shutil.rmtree(output_folder)\n",
    "        print(f\"Removed existing folder: {output_folder}\")\n",
    "    model.val(\n",
    "        data=str(data_path),\n",
    "        verbose=True,\n",
    "        device=\"cuda\",\n",
    "        split=split,\n",
    "        save_json=True,\n",
    "        project=OUTPUT_PATH,\n",
    "        name=dataset_name,\n",
    "        conf=0.25,\n",
    "        batch=2\n",
    "    )\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    json_file = f\"{output_folder}/predictions.json\"\n",
    "    filter_and_save_json(json_file)\n",
    "    # check the number of entries in the JSON file\n",
    "    if os.path.exists(json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"Number of entries in JSON file: {len(data)}\")\n",
    "    else:\n",
    "        print(\"JSON file not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.34 ðŸš€ Python-3.9.19 torch-2.0.1+cu117 CUDA:0 (NVIDIA RTX A4000 Laptop GPU, 8192MiB)\n",
      "YOLOv10s summary (fused): 293 layers, 8050440 parameters, 0 gradients, 24.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/hugo/datasets/voc/labels/train.cache... 16551 images, 1729 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16551/16551 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8276/8276 [04:04<00:00, 33.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      16551      39612      0.929      0.896      0.956      0.813\n",
      "               bicycle      16551       1208      0.947      0.894      0.954      0.818\n",
      "                  bird      16551       1820      0.946      0.917      0.974      0.817\n",
      "                  boat      16551       1397        0.9      0.797        0.9      0.698\n",
      "                bottle      16551       2116      0.889      0.814      0.914      0.696\n",
      "                   bus      16551        909      0.935      0.924      0.974      0.881\n",
      "                   car      16551       4008      0.908      0.871      0.945      0.775\n",
      "                   cat      16551       1616      0.965      0.966       0.99      0.928\n",
      "                 chair      16551       4338      0.876      0.823      0.906       0.74\n",
      "                   cow      16551       1058      0.924      0.915      0.968      0.814\n",
      "                   dog      16551       2079      0.963      0.963      0.989      0.903\n",
      "                 horse      16551       1156      0.962      0.942      0.979       0.88\n",
      "                person      16551      15576      0.928      0.878      0.955      0.781\n",
      "                 sheep      16551       1347      0.916      0.895      0.956      0.767\n",
      "                 train      16551        984      0.947      0.944      0.984      0.885\n",
      "Speed: 0.5ms preprocess, 7.8ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Saving feats/v10s_voc/voc-train/predictions.json...\n",
      "Results saved to \u001b[1mfeats/v10s_voc/voc-train\u001b[0m\n",
      "Number of entries in JSON file: 41974\n",
      "Ultralytics YOLOv8.1.34 ðŸš€ Python-3.9.19 torch-2.0.1+cu117 CUDA:0 (NVIDIA RTX A4000 Laptop GPU, 8192MiB)\n",
      "YOLOv10s summary (fused): 293 layers, 8050440 parameters, 0 gradients, 24.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/hugo/datasets/voc/labels/val.cache... 4952 images, 487 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4952/4952 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2476/2476 [01:06<00:00, 37.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4952      12648      0.846      0.773       0.85      0.644\n",
      "               bicycle       4952        389      0.869      0.799      0.869      0.674\n",
      "                  bird       4952        576      0.891      0.753      0.839      0.585\n",
      "                  boat       4952        393      0.774      0.613      0.709      0.449\n",
      "                bottle       4952        657      0.836      0.645      0.761      0.507\n",
      "                   bus       4952        254       0.83      0.783       0.87      0.743\n",
      "                   car       4952       1541      0.879      0.814      0.897      0.699\n",
      "                   cat       4952        370      0.879      0.865      0.931      0.781\n",
      "                 chair       4952       1374      0.735      0.596      0.677      0.462\n",
      "                   cow       4952        329      0.855      0.824      0.896      0.657\n",
      "                   dog       4952        530      0.881      0.811        0.9      0.734\n",
      "                 horse       4952        395      0.905      0.866      0.925      0.736\n",
      "                person       4952       5227      0.851      0.798       0.88      0.627\n",
      "                 sheep       4952        311      0.784      0.781      0.833      0.636\n",
      "                 train       4952        302      0.875      0.871      0.911       0.72\n",
      "Speed: 0.5ms preprocess, 7.5ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Saving feats/v10s_voc/voc-val/predictions.json...\n",
      "Results saved to \u001b[1mfeats/v10s_voc/voc-val\u001b[0m\n",
      "Number of entries in JSON file: 13372\n",
      "Ultralytics YOLOv8.1.34 ðŸš€ Python-3.9.19 torch-2.0.1+cu117 CUDA:0 (NVIDIA RTX A4000 Laptop GPU, 8192MiB)\n",
      "YOLOv10s summary (fused): 293 layers, 8050440 parameters, 0 gradients, 24.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/hugo/datasets/OOD-open/labels/val.cache... 0 images, 1852 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1852/1852 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ No labels found in /home/hugo/datasets/OOD-open/labels/val.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 926/926 [01:05<00:00, 14.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1852          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in detect set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.4ms preprocess, 7.6ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Saving feats/v10s_voc/OOD-open/predictions.json...\n",
      "Results saved to \u001b[1mfeats/v10s_voc/OOD-open\u001b[0m\n",
      "Number of entries in JSON file: 1861\n"
     ]
    }
   ],
   "source": [
    "id = \"voc\"\n",
    "for dataset_name in [f\"{id}-train\", f\"{id}-val\", \"OOD-open\"]:\n",
    "    inference_dataset(dataset_name, f\"v10s_{id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.34 ðŸš€ Python-3.9.19 torch-2.0.1+cu117 CUDA:0 (NVIDIA RTX A4000 Laptop GPU, 8192MiB)\n",
      "YOLOv10s summary (fused): 293 layers, 8042700 parameters, 0 gradients, 24.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/hugo/datasets/bdd/labels/val.cache... 10000 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [02:40<00:00, 31.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      10000     185945      0.732      0.507      0.567      0.335\n",
      "            pedestrian      10000      13425      0.748      0.569      0.664      0.347\n",
      "                 rider      10000        658      0.643      0.447       0.49      0.259\n",
      "                   car      10000     102837      0.827       0.74      0.829      0.541\n",
      "                 truck      10000       4243      0.701      0.599      0.664       0.49\n",
      "                   bus      10000       1660      0.701      0.584      0.653      0.509\n",
      "                 train      10000         15          1          0     0.0202     0.0177\n",
      "            motorcycle      10000        460      0.665      0.427      0.485      0.265\n",
      "               bicycle      10000       1039       0.61      0.472      0.513      0.265\n",
      "         traffic light      10000      26884      0.707       0.58      0.646      0.261\n",
      "          traffic sign      10000      34724      0.717      0.654      0.708      0.394\n",
      "Speed: 0.4ms preprocess, 6.2ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Saving /home/hugo/yolov10FX/feats/v10s_bdd/bdd-val/predictions.json...\n",
      "Results saved to \u001b[1m/home/hugo/yolov10FX/feats/v10s_bdd/bdd-val\u001b[0m\n",
      "Number of entries in JSON file: 168100\n",
      "Ultralytics YOLOv8.1.34 ðŸš€ Python-3.9.19 torch-2.0.1+cu117 CUDA:0 (NVIDIA RTX A4000 Laptop GPU, 8192MiB)\n",
      "YOLOv10s summary (fused): 293 layers, 8042700 parameters, 0 gradients, 24.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/hugo/datasets/ID-bdd-OOD-coco/labels/val.cache... 0 images, 1880 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1880/1880 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ No labels found in /home/hugo/datasets/ID-bdd-OOD-coco/labels/val.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 940/940 [00:22<00:00, 42.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1880          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in detect set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.4ms preprocess, 8.3ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Saving /home/hugo/yolov10FX/feats/v10s_bdd/ID-bdd-OOD-coco/predictions.json...\n",
      "Results saved to \u001b[1m/home/hugo/yolov10FX/feats/v10s_bdd/ID-bdd-OOD-coco\u001b[0m\n",
      "Number of entries in JSON file: 1359\n",
      "Ultralytics YOLOv8.1.34 ðŸš€ Python-3.9.19 torch-2.0.1+cu117 CUDA:0 (NVIDIA RTX A4000 Laptop GPU, 8192MiB)\n",
      "YOLOv10s summary (fused): 293 layers, 8042700 parameters, 0 gradients, 24.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/hugo/datasets/OOD-open/labels/val.cache... 0 images, 1852 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1852/1852 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ No labels found in /home/hugo/datasets/OOD-open/labels/val.cache, training may not work correctly. See https://docs.ultralytics.com/datasets/detect for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 926/926 [01:02<00:00, 14.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1852          0          0          0          0          0\n",
      "WARNING âš ï¸ no labels found in detect set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.4ms preprocess, 7.7ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Saving /home/hugo/yolov10FX/feats/v10s_bdd/OOD-open/predictions.json...\n",
      "Results saved to \u001b[1m/home/hugo/yolov10FX/feats/v10s_bdd/OOD-open\u001b[0m\n",
      "Number of entries in JSON file: 930\n"
     ]
    }
   ],
   "source": [
    "id = \"bdd\"\n",
    "for dataset_name in [f\"{id}-train\", f\"{id}-val\", f\"ID-{id}-OOD-coco\", \"OOD-open\"]:\n",
    "    inference_dataset(dataset_name, f\"v10s_{id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# postprocessing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msp_postprocess(logits):\n",
    "    return torch.max(torch.softmax(logits, dim=1), dim=1)[0]\n",
    "\n",
    "def ebo_postprocess(logits, temperature=1):\n",
    "    return temperature * torch.logsumexp(logits / temperature, dim=1)\n",
    "\n",
    "def maxlogits_postprocess(logits):\n",
    "    return torch.max(logits, dim=1)[0]\n",
    "\n",
    "def mahalanobis_compute_mean(logits):\n",
    "    num_classes = logits[0].shape[0]\n",
    "    all_labels = torch.tensor([logit.argmax(0) for logit in logits])\n",
    "    class_mean = []\n",
    "    centered_data = []\n",
    "    for c in range(num_classes):\n",
    "        class_samples = logits[all_labels.eq(c)]\n",
    "        if class_samples.size(0) > 0:\n",
    "            mean = class_samples.mean(0)\n",
    "            centered = class_samples - mean\n",
    "        else:\n",
    "            mean = torch.zeros(logits.size(1))\n",
    "            centered = torch.empty((0, logits.size(1)))\n",
    "        class_mean.append(mean)\n",
    "        centered_data.append(centered)\n",
    "\n",
    "    if len(torch.cat(centered_data)) == 0:\n",
    "        raise ValueError(\"No samples available for any class to compute covariance.\")\n",
    "    \n",
    "    group_lasso = sklearn.covariance.EmpiricalCovariance(assume_centered=False)\n",
    "    group_lasso.fit(torch.cat(centered_data).numpy().astype(np.float32))\n",
    "    precision = torch.from_numpy(group_lasso.precision_).float()\n",
    "    return class_mean, precision\n",
    "\n",
    "def mahalanobis_postprocess(logits, class_mean, precision):\n",
    "    num_classes = logits[0].shape[0]\n",
    "    precision = precision.double()\n",
    "    class_scores = torch.zeros((logits.shape[0], num_classes))\n",
    "    for c in range(num_classes):\n",
    "        if class_mean[c].numel() > 0:\n",
    "            tensor = logits - class_mean[c].double()\n",
    "            class_scores[:, c] = -torch.sum(tensor * torch.matmul(precision, tensor.t()).t(), dim=1)\n",
    "    return torch.max(class_scores, dim=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instance matching and reorganization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganize_predictions(file_path, threshold=0.25):\n",
    "    data = load_json_data(file_path)\n",
    "    reorganized_data = defaultdict(list)\n",
    "    for item in data:\n",
    "        if item['score'] >= threshold: \n",
    "            reorganized_data[item['image_id']].append({\n",
    "                'category_id': item['category_id'],\n",
    "                'bbox': item['bbox'],\n",
    "                'score': item['score'],\n",
    "                'logits': item['logits']\n",
    "            })\n",
    "    return dict(reorganized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(model_name, dataset_name, postprocess):\n",
    "    model_type, id = model_name.split('_')\n",
    "    original_file = f'feats/{model_name}/{dataset_name}/predictions.json' \n",
    "    if postprocess == mahalanobis_postprocess:\n",
    "        orginal_train_file = f\"feats/{model_name}/{id}-train/predictions.json\"\n",
    "        if not os.path.exists(orginal_train_file):\n",
    "            print(f\"Running inference for original {id}-train dataset...\")\n",
    "            predictions = inference_dataset(f'{id}-train', model_name)\n",
    "            os.makedirs(os.path.dirname(orginal_train_file), exist_ok=True)\n",
    "            with open(orginal_train_file, 'w') as f:\n",
    "                json.dump(predictions, f)\n",
    "    if not os.path.exists(original_file):\n",
    "        print(f\"Running inference for original {dataset_name} dataset...\")\n",
    "        predictions = inference_dataset(dataset_name, model_name)\n",
    "        os.makedirs(os.path.dirname(original_file), exist_ok=True)\n",
    "        with open(original_file, 'w') as f:\n",
    "            json.dump(predictions, f)\n",
    "    instance_predictions_original = reorganize_predictions(original_file)\n",
    "    original_scores_list = []\n",
    "    original_logits_list = []\n",
    "\n",
    "    original_logits_list = [det['logits'] for ins_ori in instance_predictions_original.values() for det in ins_ori]\n",
    "    if postprocess == mahalanobis_postprocess:\n",
    "        instance_predictions_original_train = reorganize_predictions(orginal_train_file)\n",
    "        original_train_logits_list = [det['logits'] for v in instance_predictions_original_train.values() for det in v]\n",
    "        class_mean, precision = mahalanobis_compute_mean(torch.tensor(original_train_logits_list))\n",
    "        original_scores_list = postprocess(torch.tensor(original_logits_list), class_mean, precision).tolist()\n",
    "    else:\n",
    "        original_scores_list = postprocess(torch.tensor(original_logits_list)).tolist()\n",
    "    \n",
    "    return original_scores_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ood metrics computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_combined_ood_metrics(model_name, id_dataset_name, ood_dataset_name):\n",
    "    postprocess_methods = {\n",
    "        'msp': msp_postprocess,\n",
    "        'ebo': ebo_postprocess,\n",
    "        'mls': maxlogits_postprocess,\n",
    "        'mds': mahalanobis_postprocess,\n",
    "    }\n",
    "\n",
    "    fpr95_results = {}\n",
    "\n",
    "    for method_name, postprocess in postprocess_methods.items():\n",
    "        id_original_scores = process_dataset(model_name, id_dataset_name, postprocess)\n",
    "        ood_original_scores = process_dataset(model_name, ood_dataset_name, postprocess)\n",
    "        conf = np.array(id_original_scores + ood_original_scores)\n",
    "        label = np.array([0]*len(id_original_scores) + [-1]*len(ood_original_scores))\n",
    "        fpr95 = compute_fpr(conf, label)\n",
    "        fpr95_results[method_name] = round(fpr95, 4)\n",
    "\n",
    "    return fpr95_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'msp': 0.6835, 'ebo': 0.9355, 'mls': 0.9264, 'mds': 0.6668}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"v10s_voc\"\n",
    "id_dataset_name = \"voc-val\"\n",
    "ood_dataset_name = \"OOD-open\"\n",
    "fpr95_results = compute_combined_ood_metrics(model_name, id_dataset_name, ood_dataset_name)\n",
    "print(fpr95_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'msp': 0.7645, 'ebo': 0.9097, 'mls': 0.8946, 'mds': 0.6054}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"v10s_bdd\"\n",
    "id_dataset_name = \"bdd-val\"\n",
    "ood_dataset_name = \"OOD-open\"\n",
    "fpr95_results = compute_combined_ood_metrics(model_name, id_dataset_name, ood_dataset_name)\n",
    "print(fpr95_results)\n",
    "# bam evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bam evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bam(model_name, density, ood_dataset_name):\n",
    "    def npy2feats_dict(model_type, id, dataset_name):\n",
    "        logits, labels = json2npy(model_type, id, dataset_name)\n",
    "        feats_dict = defaultdict(list)\n",
    "        for logit, label in zip(logits, labels):\n",
    "            feats_dict[label].append(logit)\n",
    "        return {k: np.array(v) for k, v in feats_dict.items()}\n",
    "\n",
    "    model_type, id = model_name.split('_')\n",
    "    monitor_path = f\"monitors/{model_name}_{density}.pkl\"\n",
    "    import pickle\n",
    "    if os.path.exists(monitor_path):\n",
    "        with open(monitor_path, \"rb\") as f:\n",
    "            monitor_dict = pickle.load(f)\n",
    "    else:\n",
    "        monitor_dict = {}\n",
    "        feats_dict = npy2feats_dict(model_type, id, f\"{id}-train\")\n",
    "    \n",
    "        for k, v in tqdm(feats_dict.items(), desc=\"Building monitor\"):\n",
    "            if len(v) >= density:\n",
    "                k_start = round(len(v)/density)\n",
    "                clustering_results = features_clustering_by_k_start(v, k_start)\n",
    "                monitor_dict[k] = monitor_construction_from_features(v, clustering_results)\n",
    "        # save monitor_dict\n",
    "        os.makedirs(os.path.dirname(monitor_path), exist_ok=True)\n",
    "        with open(monitor_path, \"wb\") as f:\n",
    "            pickle.dump(monitor_dict, f)\n",
    "    feats_id = npy2feats_dict(model_type, id, f\"{id}-val\")\n",
    "    distances_id = np.array([-distance for distances in get_distance_dataset(monitor_dict, feats_id).values() for distance in distances])\n",
    "    \n",
    "    feats_ood = npy2feats_dict(model_type, id, ood_dataset_name)\n",
    "    distances_ood = np.array([-distance for distances in get_distance_dataset(monitor_dict, feats_ood).values() for distance in distances])\n",
    "    \n",
    "    conf = np.concatenate([distances_id, distances_ood])\n",
    "    label = np.concatenate([np.ones(len(distances_id)), -np.ones(len(distances_ood))])\n",
    "    fpr95 = compute_fpr(conf, label)\n",
    "    return round(fpr95, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building monitor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14/14 [00:01<00:00,  9.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'msp': 0.6835, 'ebo': 0.9355, 'mls': 0.9264, 'mds': 0.6668, 'bam': 0.4836}\n"
     ]
    }
   ],
   "source": [
    "# evaluation for v10s, no perturbation applied\n",
    "ood_dataset_name = 'OOD-open'\n",
    "id_dataset_name = 'voc-val'\n",
    "model_name = \"v10s_voc\"\n",
    "density = 5\n",
    "\n",
    "fpr95_results= compute_combined_ood_metrics(model_name, id_dataset_name, ood_dataset_name)\n",
    "fpr95_bam = evaluate_bam(model_name, density, ood_dataset_name)\n",
    "fpr95_results['bam'] = fpr95_bam\n",
    "print(fpr95_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'msp': 0.5241, 'ebo': 0.9394, 'mls': 0.9184, 'mds': 0.5501, 'bam': 0.4104}\n"
     ]
    }
   ],
   "source": [
    "# evaluation for v10s, no perturbation applied\n",
    "ood_dataset_name = 'ID-voc-OOD-coco'\n",
    "id_dataset_name = 'voc-val'\n",
    "model_name = \"v10s_voc\"\n",
    "density = 5\n",
    "\n",
    "fpr95_results= compute_combined_ood_metrics(model_name, id_dataset_name, ood_dataset_name)\n",
    "fpr95_bam = evaluate_bam(model_name, density, ood_dataset_name)\n",
    "fpr95_results['bam'] = fpr95_bam\n",
    "print(fpr95_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building monitor: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [05:37<00:00, 33.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'msp': 0.7645, 'ebo': 0.9097, 'mls': 0.8946, 'mds': 0.6054, 'bam': 0.4753}\n"
     ]
    }
   ],
   "source": [
    "# evaluation for v10s, no perturbation applied\n",
    "ood_dataset_name = 'OOD-open'\n",
    "id_dataset_name = 'bdd-val'\n",
    "model_name = \"v10s_bdd\"\n",
    "density = 50\n",
    "\n",
    "fpr95_results= compute_combined_ood_metrics(model_name, id_dataset_name, ood_dataset_name)\n",
    "fpr95_bam = evaluate_bam(model_name, density, ood_dataset_name)\n",
    "fpr95_results['bam'] = fpr95_bam\n",
    "print(fpr95_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'msp': 0.7837, 'ebo': 0.9073, 'mls': 0.8904, 'mds': 0.6313, 'bam': 0.5129}\n"
     ]
    }
   ],
   "source": [
    "ood_dataset_name = 'ID-bdd-OOD-coco'\n",
    "id_dataset_name = 'bdd-val'\n",
    "model_name = \"v10s_bdd\"\n",
    "density = 50\n",
    "\n",
    "fpr95_results= compute_combined_ood_metrics(model_name, id_dataset_name, ood_dataset_name)\n",
    "fpr95_bam = evaluate_bam(model_name, density, ood_dataset_name)\n",
    "fpr95_results['bam'] = fpr95_bam\n",
    "print(fpr95_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
